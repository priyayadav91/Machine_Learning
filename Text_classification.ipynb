{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c46b781e",
   "metadata": {},
   "source": [
    "## Text Classification using MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa74a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import math\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bd7910",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop word file location you can change the path as per your convinence\n",
    "with open('C:/Users/priya/Downloads/20_newsgroups/stopwords.txt',encoding='UTF-8') as file:\n",
    "    stop_words_txt=file.read()\n",
    "    stop_words={}\n",
    "    for st_word in stop_words_txt.splitlines():\n",
    "        stop_words[st_word.lower()]=stop_words.get(st_word.lower(),0)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647281ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# location of newsgroup you can change the path as per your convinence\n",
    "rootdir=r\"C:\\Users\\priya\\Downloads\\20_newsgroups\\20_newsgroups\"\n",
    "word_freq={}\n",
    "for folder in os.listdir(rootdir):\n",
    "    new_rootdir=os.path.join(rootdir, folder)\n",
    "    for each_subfile in os.listdir(new_rootdir)[:50]:\n",
    "        file_=os.path.join(new_rootdir,each_subfile)\n",
    "        with open(file_) as file:\n",
    "            #read each file and add words to word_freq dictionary\n",
    "            file_txt=file.read()\n",
    "            file_txt=file_txt.lower()\n",
    "            # ignored text till the occurance of line: to filter the data\n",
    "            file_txt=file_txt.partition('lines:')[2]\n",
    "            for line in file_txt.splitlines():\n",
    "                for word in re.split(':|;| |,|\\n|\\/|\\.|\\t|!|-',line):\n",
    "                    word=re.sub(\"[^a-zA-Z']\", '', word)\n",
    "                    word=word.lower()\n",
    "                    if word.lower() in stop_words or word=='':\n",
    "                        pass\n",
    "                    else:\n",
    "                        word_freq[word]=word_freq.get(word,0)+1\n",
    "            file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055ea94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq=dict(sorted(word_freq.items(), key=lambda item: item[1],reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dd5711",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get top freq k words from word_freq and add then as df column\n",
    "k=3000\n",
    "column_names=list(word_freq.keys())[:k]\n",
    "column_names.append(\"_output_\")\n",
    "df=pd.DataFrame(columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aca6ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataframe():\n",
    "    \n",
    "    #built dataframe corresponding to most frequnt words\n",
    "    rootdir=r\"C:\\Users\\priya\\Downloads\\20_newsgroups\\20_newsgroups\"\n",
    "    word_freq={}\n",
    "    for folder in os.listdir(rootdir):\n",
    "        new_rootdir=os.path.join(rootdir, folder)\n",
    "        for each_subfile in os.listdir(new_rootdir):\n",
    "            file_=os.path.join(new_rootdir,each_subfile)\n",
    "            with open(file_) as file:\n",
    "                #read each file and add words to dictionary\n",
    "                file_txt=file.read()\n",
    "                row_list=[0 for i in range(k)]\n",
    "                for line in file_txt.splitlines():\n",
    "                    for word in re.split(':|;| |,|\\n|\\/|\\.|\\t|!|-',line):\n",
    "                        word=re.sub(\"[^a-zA-Z']\", '', word)\n",
    "                        word=word.lower()\n",
    "                        if word in column_names:\n",
    "                            row_list[column_names.index(word)]=row_list[column_names.index(word)]+1\n",
    "                row_list.append(folder)\n",
    "                df.loc[len(df.index)] = row_list\n",
    "                file.close()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33d9b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data=make_dataframe()\n",
    "input_data[\"_output_\"]=y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2914edee",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=input_data\n",
    "y=data[\"_output_\"]\n",
    "del data[\"_output_\"]\n",
    "x=data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ef6591",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tarin,x_test,y_tain,y_test=train_test_split(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f7919e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b57861",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(x_tarin,y_tain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0222d613",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict=clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d158518e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat=confusion_matrix(y_test,predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2b05a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2e0e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df094e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data representation using heatmap\n",
    "sns.heatmap(mat.T, square = True, annot=True, fmt = \"d\", xticklabels=list(set(y)),yticklabels=list(set(y)))\n",
    "plt.xlabel(\"true labels\")\n",
    "plt.ylabel(\"predicted label\")\n",
    "plt.show()\n",
    "# print(\"The accuracy is {}\".format(accuracy_score(test_data.target, predicted_categories)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7852041a",
   "metadata": {},
   "source": [
    "## Custom implementation of Text Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429770a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionary(train,column_names):\n",
    "    unique_classes=set(train[\"_output_\"])\n",
    "    unique_features=column_names\n",
    "    unique_features.pop()\n",
    "    dict_class={}\n",
    "    for class_ in unique_classes: \n",
    "\n",
    "        # build a dictionary of features with respect to each class\n",
    "        data_corresponding_class=train[train[\"_output_\"]==class_]        \n",
    "        dict_class[class_]={}\n",
    "    #     print(data_corresponding_class)\n",
    "        del data_corresponding_class[\"_output_\"]\n",
    "    #     print(data_corresponding_class)\n",
    "        total_sum=data_corresponding_class.sum().sum()\n",
    "    #     print(total_sum)\n",
    "        for feature in unique_features:\n",
    "            dict_class[class_][feature]=calculate_probablity(data_corresponding_class[feature].sum(),total_sum) # calculate probablity\n",
    "    return dict_class\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0abd9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating probablity for each feature\n",
    "def calculate_probablity(favourable,total_outcomes):\n",
    "    prob=math.log((favourable+1)/(total_outcomes+3000))\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a457fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dictionary of feature probablity with respecct to the calss\n",
    "dict_=create_dictionary(input_data,column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7a4332",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict the output for the given test cases\n",
    "def predictclass(dict_class,test_data,unique_classes):\n",
    "#     del test_data[\"_output_\"]\n",
    "    predicted_output=[]\n",
    "    for ele in test_data:\n",
    "#         np_ele=np.array(ele)\n",
    "        max_prob=-9999999\n",
    "        class_to_be_selected=\"\"\n",
    "        pd_class_dict=pd.DataFrame.from_dict(dict_class)\n",
    "        for each_class in unique_classes:\n",
    "            a=np.array(pd_class_dict[each_class])\n",
    "#             print(a)\n",
    "            prob=((a)*(ele)).sum()\n",
    "            if max_prob<prob:\n",
    "                max_prob=prob\n",
    "                class_to_be_selected=each_class\n",
    "        predicted_output.append(class_to_be_selected)\n",
    "    return predicted_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90758538",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting dataframe to numpy array\n",
    "p=x_test\n",
    "d=p.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028a07f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted output using custom Text Classification\n",
    "out=predictclass(dict_,d,set(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4a7de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test,out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb21dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating confusion matrix \n",
    "m=confusion_matrix(y_test,out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563b200d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drawing a heatmap for \n",
    "sns.heatmap(m.T, square = True, annot=True, fmt = \"d\", xticklabels=list(set(y)),yticklabels=list(set(y)))\n",
    "plt.xlabel(\"true labels\")\n",
    "plt.ylabel(\"predicted label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8cdb5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
